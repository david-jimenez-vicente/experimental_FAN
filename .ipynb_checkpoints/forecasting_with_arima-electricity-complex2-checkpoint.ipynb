{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc400e4-0f12-4029-89ae-5a60ccf2dc22",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">TIME SERIES FORECASTING CON REDES EXPERIMENTALES FAN (Fourier Analysis Networks)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd3e03-382d-4a1d-b28c-7264e26e06b4",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">Autor: José David Jiménez Vicente</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e8011b-5d8a-4358-9a3f-f8015ae3d221",
   "metadata": {},
   "source": [
    "____  \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab3fcfa-ba33-413b-936e-c3dba9f51323",
   "metadata": {},
   "source": [
    "#### MOTIVACIÓN:\n",
    "  \n",
    "  Este es un proyecto experimental para incorporar un nuevo tipo de redes neuronales a un proyecto de forecasting de series temporales, las redes FAN, que están basadas en un nuevo algoritmo basado en las Series de Fourier para predecir la función que queremos descubrir en base a los datos, en vez de usar combinaciones lineales por muy complejas que sean como algoritmo. Puedes ver el paper completo de los creadores [aquí](https://arxiv.org/html/2410.02675v3). Mi objetivo no es hacer un estudio matemático del proyecto, para eso ya está el propio proyecto, si no traducir las matemáticas a un lenguaje lo más natural posible para revelar su intuición y cómo aplicarlo en la realidad. Que me perdonen los matemáticos y los creadores del proyecto original por no ser matemáticamente exacto, pero lo que han creado es tan bello que hay que intentar explicarlo de manera que llegue a cuantos más público sea posible.    \n",
    "  _____\n",
    "#### ANÁLISIS:  \n",
    "  Podemos resumir el motivo de existencia de estas redes de la siguiente manera:\n",
    "  \n",
    "- Los algoritmos habituales usados para redes como LSTM, ARIMA o muchos de los Transformers no dejan de ser más que combinaciones lineales especializadas con una activación no lineal, sobrecargadas de variables y sus pesos, que se hacen crecer en complejidad de capas, y con uso de datos masivos para poder llegar a aproximar de forma indirecta la función que queremos predecir, como \"tanteando a ciegas\" para encontrar la solución. El \"tanteo\" lo hacen intentando reducir el error cometido (RMSE, MSE, ....). Si reducen dicho error, es que lo están haciendo bien, hasta que ya no pueden más.\n",
    "- La ventaja de las FAN según sus creadores es que le aportan a la red la capacidad de detectar patrones temporales oscilatorios dentro del propio proceso de aprendizaje del algoritmo, al incorporar funciones de seno y coseno dentro de la fórmula donde antes sólo había una combinación lineal. Es decir, en vez de \"tantear a ciegas\" la solución en base a si reducen el error, las FAN le dan una capacidad natural comparable a \"sentir\" la función oscilatoria mientras aprenden para poder acercarse a ella. Sus caraterísticas y ventajas se ponen enseguida de manifiesto:\n",
    "\n",
    "  - Son muchísimo más exactas donde no hay, y donde si hay, datos previos: fuera del periodo temporal que se ha usado para el entrenamiento. Por lo tanto, generalizan mejor.\n",
    "  - Les lleva mucho menos tiempo converger.\n",
    "  - Necesitan menos pesos, con lo que acaban construyendo modelos más ligeros.\n",
    "  - Al \"sentir\" de manera natural los patrones, pueden detectar estacionalidad o patrones ocultos donde pensamos que no los puede haber. Por lo tanto, son buenas para poder detectar conmo filtro previo si un dataset puede contener patrones repetitivos donde ni lo habíamos planteado en principio.\n",
    "  - Fruto de lo anterior, seguramente sean muy buenas para implementar LLMs, ya que el lenguaje no es más que patrones de comunicación con reglas repetitivas establecidas. Si no fuera así, no podríamos interpretar ninguna comunicación. Incluso, las comunicaciones con cifrados clásicos basados en transmutación del lenguaje, como el César.\n",
    "  - Incorporan una parte lineal dentro del algoritmo, por lo que para problemas no oscilatorios son tan buenas como, o más que, un algoritmo lineal habitual, por lo que se pueden usar para cualquier tarea.\n",
    "  - Las capas de FAN se pueden implementar fácilmente con librerías como pyTorch, integrándolas dentro de otros modelos, como por ejemplo un Transformer, para potenciar su resultado.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4eded-b82c-4dd3-99b5-eeba13773156",
   "metadata": {},
   "source": [
    "#### PRIMER VISTAZO:\n",
    "\n",
    "Aquí podéis una gráfica sacada del paper en ArXiv para que entendáis visualmente a lo que me refiero:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494575e8-2d8f-431b-970d-95220a77e60f",
   "metadata": {},
   "source": [
    "![Comparativa](https://arxiv.org/html/2410.02675v3/x1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c882c772-5c95-495a-8e44-08aa04933bff",
   "metadata": {},
   "source": [
    "__*Explicación de la gráfica:*__  \n",
    "  - Se comparan en el paper original una red tradicional multicapa Multi-Layer Perceptron (MLP), una red Kolmogorov-Arnold (KAN), una red Transformer como las que alimentan a los LLM, y una FAN.\n",
    "  - Aunque no se aprecie al mezclarse con el color rojo de los modelos dando el marrón, las líneas verdes simbolizan el periodo de datos suministrado para el entrenamiento, y van del -20 al +20 en el eje X. Las azules son aquellas de datos desconocidos para el modelo, suministrados para el test tras el entrenamiento.\n",
    "  - Podemos ver que las lineas rojas son las predicciones de los modelos. En todos, la línea se vuelve marrón donde los modelos coinciden en la predicción con los datos que conocen en color verde. En los modelos no FAN, coinciden correctamente en las predicciones sobre los datos de entrenamiento, pero más allá son incapaces de capturar la función de los ciclos no lineales, al estar basados en relaciones lineales. Sin embargo, el resultado de la FAN es abrumador: ha capturado el patrón y puede repetir la predicción sobre los datos que no conoce perfectamente. Podríamos pensar: \"ja, es que sólo es un seno\"; sin embargo los otros modelos no han podido capturar un ciclo tan estable y predecible como el del seno.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef979819-48df-4206-bc05-80f95df25e85",
   "metadata": {},
   "source": [
    "¿Y si le damos a tragar algo no tan fácil como un seno?:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5c2510-7743-49b7-9525-c87e2daff4e7",
   "metadata": {},
   "source": [
    "![Función compleja](https://arxiv.org/html/2410.02675v3/x4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd003a-3178-4d70-8d4e-a040d20b51d0",
   "metadata": {},
   "source": [
    "Símplemente, deja con la boca abierta. Esta es una función compleja, con múltiples ciclos dentro de ciclos. Los algoritmos lineales en este caso no son capaces ni de predecir correctamente todos los datos que se han usado para entrenar al modelo. El que más acierta es el Transformer, de ahí sus grandes resultados en las aplicaciones como LLMs y generativa, pero la red FAN predice el ciclo completo, al ser repetitivo.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9782d21-2062-4f02-a08b-3de609e0eccb",
   "metadata": {},
   "source": [
    "#### VALE, PERO ¿CÓMO SE COMPORTAN REALMENTE?:\n",
    "\n",
    "En el estudio original comparan el rendimiento de los distintos modelos mediante la función de pérdida, aproximando 4 funciones distintas y mostrando el loss en training y en testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8ce42-e6ee-4cdb-be61-80e68c6aeb37",
   "metadata": {},
   "source": [
    "![Comparativa](https://arxiv.org/html/2410.02675v3/x5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f50047-e14d-40b2-bad6-ba11ad776c1e",
   "metadata": {},
   "source": [
    "Increíble, ¿no?. Casi no hay que añadir palabras. Los errores del FAN son casi tan próximos a 0 en algunas gráficas que se confunden con el eje X. Las KAN son lás únicas que se les pueden aproximar en cuanto a estabilidad, aunque quedan igualmente lejos en cuanto a resultado.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b29b722-7957-4d09-aa3c-4e1614accf96",
   "metadata": {},
   "source": [
    "#### CHUPITO MATEMÁTICO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a8f941-2c8d-4641-9eca-70f90b029f40",
   "metadata": {},
   "source": [
    "No quiero entrar en las matemáticas subyacentes, como decía, pero no queda más remedio que dar un pequeño brochazo de color matemático:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1257b2-b547-425d-91d1-4a2376676dcb",
   "metadata": {},
   "source": [
    "![Diferencia básica entre modelos](https://arxiv.org/html/2410.02675v3/x2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e075a-9c45-4e14-8008-d43a445aef4f",
   "metadata": {},
   "source": [
    "Si observamos con atención, en las redes habituales como la MLP (a la izquierda), la salida de cada neurona sólo es una relación lineal modificada por una función de activación no-lineal \"σ\", como se ve en la fómula superior. Pero a la derecha vemos el motor de las FAN: Una parte ajusta un coseno, otra un seno, y otra una lineal con activación, encadenándolas en una única salida final. Por eso es tan potente.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fb6302-791e-4d47-9057-71b7ed58ef3a",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434bc9b4-2c94-4e87-ace3-6282461c7dcf",
   "metadata": {},
   "source": [
    "#### MANOS A LA OBRA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9954ef49-c402-42c4-a804-b6ef6c75c6b6",
   "metadata": {},
   "source": [
    "Es muy bonito ver lo que nos dicen los autores habiendo probado con funciones matemáticas, que, al final, son estables y predecibles a pesar de su complejidad. Pero.... ¿cómo se comportarán con datos reales, que aunque pueden tener patrones estacionales, dichos ciclos son más imprevisibles (que una función descriptible con una fórmula exacta) al poder verse afectados externamente por otras variables de comportamiento no exacto, como el clima?  \n",
    "  \n",
    "Vamos a hacerlas sudar un poco.  \n",
    "  \n",
    "Voy a testear las redes FAN aplicándolas a la predicción de consumo energético de un hogar, en base a los datos recogidos a lo largo de 4 años. Es un dataset perfecto para poner a prueba las redes FAN. El [dataset que usaré](https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption) es del magnífico repositorio para ML de la UC Irvine. Aconsejo encarecidamente darse una vualta por la descripción del datset y por el site en general.  \n",
    "  \n",
    "La red FAN puede tener una variante \"Gated\", esto es, una puerta que da más peso o a la parte cíclica o a la parte lineal dependiendo de sus importancias en cada época. Esto amplía todavía más su exactitud.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0189963-52db-49f4-a073-2c1c9b1a4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# 1. Imports y configuración de dispositivo MPS/CPU\n",
    "# ================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importar el dataset desde UCI ML Repository\n",
    "from ucimlrepo import fetch_ucirepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e0aebf-c656-45c2-8d09-3a0794854605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo MPS (Apple Silicon)\n"
     ]
    }
   ],
   "source": [
    "# Detectar y usar MPS en macOS si está disponible\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Usando dispositivo MPS (Apple Silicon)\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Usando dispositivo CUDA (NVIDIA GPU)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Usando dispositivo CPU\")\n",
    "\n",
    "#DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84690533-8422-4491-93ca-4d0c91d33ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configurar PyTorch para evitar problemas de importación circular\n",
    "torch._C._jit_set_profiling_executor(False)\n",
    "torch._C._jit_set_profiling_mode(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf491178-ff12-49e4-8d89-40d3c244fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialización mejorada para modelos FAN\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif hasattr(m, 'Wp'):\n",
    "        nn.init.xavier_uniform_(m.Wp)\n",
    "        if hasattr(m, 'Wp_bar'):\n",
    "            nn.init.xavier_uniform_(m.Wp_bar)\n",
    "            nn.init.zeros_(m.Bp_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82249370-d13e-43a6-ad9d-59af254e39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para comprobar estacionariedad\n",
    "def check_stationarity(series):\n",
    "    result = adfuller(series)\n",
    "    print('Test ADF:')\n",
    "    print(f'Estadístico: {result[0]}')\n",
    "    print(f'p-valor: {result[1]}')\n",
    "    print(f'Valores críticos: {result[4]}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"La serie es estacionaria\")\n",
    "    else:\n",
    "        print(\"La serie no es estacionaria\")\n",
    "    return result[1] <= 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ca642d-da95-4e18-804d-01500d23312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función modificada para usar SARIMA con parámetros fijos, menos exigente en recursos\n",
    "def train_and_predict_sarima_improved(train_data, test_data, exog_train=None, exog_test=None):\n",
    "    \"\"\"\n",
    "    Entrena un modelo SARIMA simplificado y realiza predicciones.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "    import time\n",
    "    \n",
    "    print(\"\\nEntrenando modelo SARIMA simplificado...\")\n",
    "    start_time = time.time()  # Iniciar contador de tiempo\n",
    "    \n",
    "    # Usar parámetros simples fijos sin búsqueda automática\n",
    "    order = (1, 1, 0)  # AR(1), diferenciación(1), MA(0) - Más simple\n",
    "    seasonal_order = (1, 0, 0, 24)  # SAR(1), Sdif(0), SMA(0), período(24) - Más simple\n",
    "    \n",
    "    try:\n",
    "        sarima_model = SARIMAX(train_data,\n",
    "                              exog=exog_train,\n",
    "                              order=order,\n",
    "                              seasonal_order=seasonal_order,\n",
    "                              enforce_stationarity=False,\n",
    "                              enforce_invertibility=False)\n",
    "        \n",
    "        # Reducir maxiter y ajustar otras configuraciones para mayor velocidad\n",
    "        sarima_fit = sarima_model.fit(disp=False, maxiter=20)\n",
    "        print(\"\\nModelo SARIMA ajustado con parámetros simplificados\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al ajustar SARIMA: {e}\")\n",
    "        print(\"Intentando con un modelo aún más simple...\")\n",
    "        \n",
    "        # Modelo aún más simple como fallback\n",
    "        order = (1, 1, 0)\n",
    "        seasonal_order = (0, 0, 0, 0)  # Sin componente estacional\n",
    "        \n",
    "        sarima_model = SARIMAX(train_data,\n",
    "                              exog=exog_train,\n",
    "                              order=order,\n",
    "                              seasonal_order=seasonal_order,\n",
    "                              enforce_stationarity=False,\n",
    "                              enforce_invertibility=False)\n",
    "        \n",
    "        sarima_fit = sarima_model.fit(disp=False, maxiter=10)\n",
    "        print(\"\\nModelo SARIMA ajustado con parámetros extremadamente simplificados\")\n",
    "    \n",
    "    # Calcular tiempo de entrenamiento\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nResumen del modelo SARIMA:\")\n",
    "    print(f\"Orden: {order}, Orden estacional: {seasonal_order}\")\n",
    "    print(f\"AIC: {sarima_fit.aic}, BIC: {sarima_fit.bic}\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos ({training_time/60:.2f} minutos)\")\n",
    "    \n",
    "    # Hacer predicciones con variables exógenas si están disponibles\n",
    "    forecast = sarima_fit.forecast(steps=len(test_data), exog=exog_test)\n",
    "    \n",
    "    return forecast, sarima_fit, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23e4f220-5b45-4fdf-9e4c-2017c245c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función mejorada para análisis detallado de resultados (solo RMSE y R²)\n",
    "def analyze_results(models, test_predictions, test_dates, test_target, arima_forecast=None):\n",
    "    \"\"\"\n",
    "    Realiza un análisis detallado de los resultados de los modelos.\n",
    "    \n",
    "    Args:\n",
    "        models: Diccionario con los modelos entrenados\n",
    "        test_predictions: Diccionario con predicciones (pred, actual) para cada modelo\n",
    "        test_dates: Fechas del conjunto de test\n",
    "        test_target: Valores reales del target en test\n",
    "        arima_forecast: Predicciones del modelo SARIMA (opcional)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    # 1. Métricas detalladas para cada modelo\n",
    "    print(\"\\n=== ANÁLISIS DETALLADO DE RESULTADOS ===\\n\")\n",
    "    \n",
    "    # Crear DataFrame para comparación de métricas\n",
    "    metrics = []\n",
    "    \n",
    "    # Agregar modelos de redes neuronales\n",
    "    for name, (preds, actuals) in test_predictions.items():\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "        r2 = r2_score(actuals, preds)\n",
    "        \n",
    "        metrics.append({\n",
    "            'Modelo': name,\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2\n",
    "        })\n",
    "    \n",
    "    # Agregar SARIMA si está disponible\n",
    "    if arima_forecast is not None:\n",
    "        rmse = np.sqrt(mean_squared_error(test_target, arima_forecast))\n",
    "        r2 = r2_score(test_target, arima_forecast)\n",
    "        \n",
    "        metrics.append({\n",
    "            'Modelo': 'SARIMA',\n",
    "            'RMSE': rmse,\n",
    "            'R²': r2\n",
    "        })\n",
    "    \n",
    "    # Convertir a DataFrame y mostrar\n",
    "    metrics_df = pd.DataFrame(metrics).set_index('Modelo')\n",
    "    print(\"Métricas de evaluación:\")\n",
    "    print(metrics_df)\n",
    "    \n",
    "    # 2. Visualización de métricas\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # RMSE\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=metrics_df.index, y=metrics_df['RMSE'])\n",
    "    plt.title('RMSE por Modelo', fontsize=14)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # R²\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=metrics_df.index, y=metrics_df['R²'])\n",
    "    plt.title('R² por Modelo', fontsize=14)\n",
    "    plt.ylabel('R²', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metricas_comparativas.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Análisis de errores por franja horaria\n",
    "    error_analysis = []\n",
    "    \n",
    "    for name, (preds, actuals) in test_predictions.items():\n",
    "        # Calcular errores\n",
    "        errors = np.abs(preds - actuals)\n",
    "        \n",
    "        # Agregar a DataFrame junto con horas\n",
    "        hours = [date.hour for date in test_dates]\n",
    "        \n",
    "        for hour, error in zip(hours, errors):\n",
    "            error_analysis.append({\n",
    "                'Modelo': name,\n",
    "                'Hora': hour,\n",
    "                'Error': error\n",
    "            })\n",
    "    \n",
    "    # Agregar SARIMA si está disponible\n",
    "    if arima_forecast is not None:\n",
    "        errors = np.abs(arima_forecast - test_target)\n",
    "        \n",
    "        for hour, error in zip(hours, errors):\n",
    "            error_analysis.append({\n",
    "                'Modelo': 'SARIMA',\n",
    "                'Hora': hour,\n",
    "                'Error': error\n",
    "            })\n",
    "    \n",
    "    # Convertir a DataFrame\n",
    "    error_df = pd.DataFrame(error_analysis)\n",
    "    \n",
    "    # Agrupar por modelo y hora\n",
    "    hourly_errors = error_df.groupby(['Modelo', 'Hora'])['Error'].mean().reset_index()\n",
    "    \n",
    "    # Visualizar errores por hora\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.lineplot(data=hourly_errors, x='Hora', y='Error', hue='Modelo', marker='o')\n",
    "    plt.title('Error Promedio por Hora del Día', fontsize=16)\n",
    "    plt.xlabel('Hora', fontsize=14)\n",
    "    plt.ylabel('Error Absoluto Medio', fontsize=14)\n",
    "    plt.xticks(range(0, 24))\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(title='Modelo')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('errores_por_hora.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Análisis de residuos\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    model_count = len(test_predictions)\n",
    "    if arima_forecast is not None:\n",
    "        model_count += 1\n",
    "    \n",
    "    plot_idx = 1\n",
    "    \n",
    "    for name, (preds, actuals) in test_predictions.items():\n",
    "        residuals = actuals - preds\n",
    "        \n",
    "        plt.subplot(model_count, 2, plot_idx)\n",
    "        plt.hist(residuals, bins=50, alpha=0.7)\n",
    "        plt.title(f'Distribución de Residuos - {name}', fontsize=12)\n",
    "        plt.xlabel('Residuo', fontsize=10)\n",
    "        plt.ylabel('Frecuencia', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(model_count, 2, plot_idx + 1)\n",
    "        plt.scatter(preds, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "        plt.title(f'Residuos vs Predicciones - {name}', fontsize=12)\n",
    "        plt.xlabel('Predicción', fontsize=10)\n",
    "        plt.ylabel('Residuo', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plot_idx += 2\n",
    "    \n",
    "    # Agregar SARIMA si está disponible\n",
    "    if arima_forecast is not None:\n",
    "        residuals = test_target - arima_forecast\n",
    "        \n",
    "        plt.subplot(model_count, 2, plot_idx)\n",
    "        plt.hist(residuals, bins=50, alpha=0.7)\n",
    "        plt.title('Distribución de Residuos - SARIMA', fontsize=12)\n",
    "        plt.xlabel('Residuo', fontsize=10)\n",
    "        plt.ylabel('Frecuencia', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(model_count, 2, plot_idx + 1)\n",
    "        plt.scatter(arima_forecast, residuals, alpha=0.5)\n",
    "        plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "        plt.title('Residuos vs Predicciones - SARIMA', fontsize=12)\n",
    "        plt.xlabel('Predicción', fontsize=10)\n",
    "        plt.ylabel('Residuo', fontsize=10)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('analisis_residuos.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 5. Correlación entre predicciones de diferentes modelos\n",
    "    print(\"\\nCorrelación entre predicciones de modelos:\")\n",
    "    \n",
    "    # Crear DataFrame con predicciones de todos los modelos\n",
    "    pred_comparison = {}\n",
    "    \n",
    "    for name, (preds, _) in test_predictions.items():\n",
    "        pred_comparison[name] = preds\n",
    "    \n",
    "    if arima_forecast is not None:\n",
    "        pred_comparison['SARIMA'] = arima_forecast\n",
    "    \n",
    "    pred_df = pd.DataFrame(pred_comparison)\n",
    "    \n",
    "    # Calcular y mostrar matriz de correlación\n",
    "    corr_matrix = pred_df.corr()\n",
    "    print(corr_matrix)\n",
    "    \n",
    "    # Visualizar matriz de correlación\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlación entre Predicciones de Modelos', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('correlacion_modelos.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 6. Análisis de predicciones extremas (picos altos y bajos)\n",
    "    print(\"\\nAnálisis de predicciones en valores extremos:\")\n",
    "    \n",
    "    # Identificar valores extremos (percentiles 5 y 95)\n",
    "    low_threshold = np.percentile(test_target, 5)\n",
    "    high_threshold = np.percentile(test_target, 95)\n",
    "    \n",
    "    low_indices = np.where(test_target <= low_threshold)[0]\n",
    "    high_indices = np.where(test_target >= high_threshold)[0]\n",
    "    \n",
    "    # Calcular RMSE en valores extremos para cada modelo\n",
    "    extremes_analysis = []\n",
    "    \n",
    "    for name, (preds, actuals) in test_predictions.items():\n",
    "        # RMSE en valores bajos\n",
    "        low_rmse = np.sqrt(mean_squared_error(actuals[low_indices], preds[low_indices]))\n",
    "        \n",
    "        # RMSE en valores altos\n",
    "        high_rmse = np.sqrt(mean_squared_error(actuals[high_indices], preds[high_indices]))\n",
    "        \n",
    "        extremes_analysis.append({\n",
    "            'Modelo': name,\n",
    "            'RMSE_Valores_Bajos': low_rmse,\n",
    "            'RMSE_Valores_Altos': high_rmse\n",
    "        })\n",
    "    \n",
    "    # Agregar SARIMA si está disponible\n",
    "    if arima_forecast is not None:\n",
    "        # RMSE en valores bajos\n",
    "        low_rmse = np.sqrt(mean_squared_error(test_target[low_indices], arima_forecast[low_indices]))\n",
    "        \n",
    "        # RMSE en valores altos\n",
    "        high_rmse = np.sqrt(mean_squared_error(test_target[high_indices], arima_forecast[high_indices]))\n",
    "        \n",
    "        extremes_analysis.append({\n",
    "            'Modelo': 'SARIMA',\n",
    "            'RMSE_Valores_Bajos': low_rmse,\n",
    "            'RMSE_Valores_Altos': high_rmse\n",
    "        })\n",
    "    \n",
    "    # Convertir a DataFrame y mostrar\n",
    "    extremes_df = pd.DataFrame(extremes_analysis).set_index('Modelo')\n",
    "    print(extremes_df)\n",
    "    \n",
    "    # Visualizar errores en valores extremos\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x=extremes_df.index, y=extremes_df['RMSE_Valores_Bajos'])\n",
    "    plt.title('RMSE en Valores Bajos', fontsize=14)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x=extremes_df.index, y=extremes_df['RMSE_Valores_Altos'])\n",
    "    plt.title('RMSE en Valores Altos', fontsize=14)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('errores_valores_extremos.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics_df, error_df, extremes_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17ffde50-37a0-4159-a531-d182ae9ac673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear gráficos interactivos\n",
    "def plot_interactive_predictions(test_dates, test_target, test_predictions, sarima_forecast=None, period=\"week\", widget_id=None):\n",
    "    \"\"\"\n",
    "    Crea gráficos interactivos con checkboxes para seleccionar modelos.\n",
    "    \n",
    "    Args:\n",
    "        test_dates: Fechas del conjunto de test\n",
    "        test_target: Valores reales del target en test\n",
    "        test_predictions: Diccionario con predicciones (pred, actual) para cada modelo\n",
    "        sarima_forecast: Predicciones del modelo SARIMA (opcional)\n",
    "        period: \"week\" para 1 semana, \"detail\" para 3 días\n",
    "        widget_id: Identificador único para los widgets (para evitar conflictos)\n",
    "    \"\"\"\n",
    "    # Determinar los puntos a mostrar\n",
    "    if period == \"week\":\n",
    "        display_points = 168  # 1 semana\n",
    "        start_idx = 0\n",
    "        title = 'Predicción de Consumo Eléctrico (1 semana)'\n",
    "        filename = 'predicciones_semana_interactivo.png'\n",
    "    else:  # \"detail\"\n",
    "        start_idx = 24  # Comenzar después del primer día\n",
    "        display_points = 72  # 3 días\n",
    "        title = 'Detalle de la Predicción (3 días)'\n",
    "        filename = 'predicciones_detalle_interactivo.png'\n",
    "    \n",
    "    # Crear lista de modelos disponibles\n",
    "    models_list = list(test_predictions.keys())\n",
    "    if sarima_forecast is not None:\n",
    "        models_list.append('SARIMA')\n",
    "    \n",
    "    # Cada instancia necesita sus propios widgets con IDs únicos\n",
    "    widget_id = widget_id or period\n",
    "    widget_prefix = f\"widget_{widget_id}_\"\n",
    "    \n",
    "    # Crear widgets para seleccionar modelos con IDs únicos\n",
    "    checkboxes = {model: widgets.Checkbox(\n",
    "                     value=True, \n",
    "                     description=model,\n",
    "                     layout=widgets.Layout(width='150px')\n",
    "                 ) for model in models_list}\n",
    "    \n",
    "    # Crear un contenedor para la figura\n",
    "    fig_output = widgets.Output()\n",
    "    \n",
    "    # Función para actualizar el gráfico basado en selecciones\n",
    "    def update_plot(b):\n",
    "        # Limpiar la salida anterior\n",
    "        fig_output.clear_output(wait=True)\n",
    "        \n",
    "        # Mostrar nueva figura\n",
    "        with fig_output:\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            \n",
    "            # Siempre mostrar datos reales\n",
    "            plt.plot(\n",
    "                test_dates[start_idx:start_idx+display_points], \n",
    "                test_target[start_idx:start_idx+display_points], \n",
    "                'k-', label='Datos reales', alpha=0.7\n",
    "            )\n",
    "            \n",
    "            # Mostrar modelos seleccionados\n",
    "            for model, checkbox in checkboxes.items():\n",
    "                if checkbox.value:  # Solo si está seleccionado\n",
    "                    if model == 'SARIMA' and sarima_forecast is not None:\n",
    "                        plt.plot(\n",
    "                            test_dates[start_idx:start_idx+display_points], \n",
    "                            sarima_forecast[start_idx:start_idx+display_points], \n",
    "                            '-.', label=f'{model} predicción', alpha=0.7\n",
    "                        )\n",
    "                    elif model in test_predictions:\n",
    "                        preds, _ = test_predictions[model]\n",
    "                        plt.plot(\n",
    "                            test_dates[start_idx:start_idx+display_points], \n",
    "                            preds[start_idx:start_idx+display_points], \n",
    "                            '--', label=f'{model} predicción', alpha=0.7\n",
    "                        )\n",
    "            \n",
    "            plt.title(title, fontsize=16)\n",
    "            plt.xlabel('Fecha', fontsize=14)\n",
    "            plt.ylabel('Consumo Global (kW)', fontsize=14)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Crear el botón de actualización con ID único\n",
    "    update_button = widgets.Button(\n",
    "        description=\"Actualizar Gráfico\",\n",
    "        layout=widgets.Layout(width='200px')\n",
    "    )\n",
    "    update_button.on_click(update_plot)\n",
    "    \n",
    "    # Crear un layout para organizar los widgets\n",
    "    checkbox_layout = widgets.VBox(list(checkboxes.values()))\n",
    "    control_layout = widgets.HBox([checkbox_layout, update_button])\n",
    "    \n",
    "    # Mostrar los controles y el gráfico inicial\n",
    "    display(widgets.HTML(f\"<h3>{title}</h3>\"))\n",
    "    display(control_layout)\n",
    "    display(fig_output)\n",
    "    \n",
    "    # Actualizar el gráfico inicial\n",
    "    update_plot(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd7540d-8f92-4f90-bc62-ac77ab14038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de la capa FAN sin usar herencia de PyTorch directamente\n",
    "class FANLayer(nn.Module):\n",
    "    def __init__(self, in_features, d_p, d_p_bar, activation=None):\n",
    "        super(FANLayer, self).__init__()\n",
    "        self.Wp = nn.Parameter(torch.randn(in_features, d_p))\n",
    "        self.Wp_bar = nn.Parameter(torch.randn(in_features, d_p_bar))\n",
    "        self.Bp_bar = nn.Parameter(torch.zeros(d_p_bar))\n",
    "        self.activation = nn.GELU() if activation is None else activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flat = x.view(batch_size * seq_len, -1)\n",
    "        \n",
    "        cos_term = torch.cos(x_flat @ self.Wp)\n",
    "        sin_term = torch.sin(x_flat @ self.Wp)\n",
    "        non_periodic = self.activation(x_flat @ self.Wp_bar + self.Bp_bar)\n",
    "        \n",
    "        combined = torch.cat([cos_term, sin_term, non_periodic], dim=-1)\n",
    "        return combined.view(batch_size, seq_len, -1)\n",
    "\n",
    "# Modelo FAN simple\n",
    "class FAN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=1):\n",
    "        super(FAN, self).__init__()\n",
    "        \n",
    "        d_p = hidden_size // 4  # dimensión periódica\n",
    "        d_p_bar = hidden_size // 2  # dimensión no periódica\n",
    "        total_dim = 2 * d_p + d_p_bar  # dimensión combinada\n",
    "        \n",
    "        # Primera capa\n",
    "        self.layer1 = FANLayer(input_size, d_p, d_p_bar)\n",
    "        \n",
    "        # Capas intermedias\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(FANLayer(total_dim, d_p, d_p_bar))\n",
    "        \n",
    "        # Capa final\n",
    "        self.output_layer = nn.Linear(total_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Tomar el último paso temporal\n",
    "        last_step = x[:, -1, :]\n",
    "        return self.output_layer(last_step)\n",
    "\n",
    "# Versión GatedFAN simplificada\n",
    "class GatedFANLayer(nn.Module):\n",
    "    def __init__(self, in_features, d_p, d_p_bar, activation=None):\n",
    "        super(GatedFANLayer, self).__init__()\n",
    "        self.Wp = nn.Parameter(torch.randn(in_features, d_p))\n",
    "        self.Wp_bar = nn.Parameter(torch.randn(in_features, d_p_bar))\n",
    "        self.Bp_bar = nn.Parameter(torch.zeros(d_p_bar))\n",
    "        self.gate = nn.Parameter(torch.zeros(1))\n",
    "        self.activation = nn.GELU() if activation is None else activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x_flat = x.view(batch_size * seq_len, -1)\n",
    "        \n",
    "        cos_term = torch.cos(x_flat @ self.Wp)\n",
    "        sin_term = torch.sin(x_flat @ self.Wp)\n",
    "        non_periodic = self.activation(x_flat @ self.Wp_bar + self.Bp_bar)\n",
    "        \n",
    "        gate = torch.sigmoid(self.gate)\n",
    "        cos_term = gate * cos_term\n",
    "        sin_term = gate * sin_term\n",
    "        non_periodic = (1 - gate) * non_periodic\n",
    "        \n",
    "        combined = torch.cat([cos_term, sin_term, non_periodic], dim=-1)\n",
    "        return combined.view(batch_size, seq_len, -1)\n",
    "\n",
    "# Modelo GatedFAN simplificado\n",
    "class GatedFAN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=1):\n",
    "        super(GatedFAN, self).__init__()\n",
    "        \n",
    "        d_p = hidden_size // 4\n",
    "        d_p_bar = hidden_size // 2\n",
    "        total_dim = 2 * d_p + d_p_bar\n",
    "        \n",
    "        # Primera capa\n",
    "        self.layer1 = GatedFANLayer(input_size, d_p, d_p_bar)\n",
    "        \n",
    "        # Capas intermedias\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(GatedFANLayer(total_dim, d_p, d_p_bar))\n",
    "        \n",
    "        # Capa final\n",
    "        self.output_layer = nn.Linear(total_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Tomar el último paso temporal\n",
    "        last_step = x[:, -1, :]\n",
    "        return self.output_layer(last_step)\n",
    "\n",
    "# Modelo LSTM simplificado\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.linear(out[:, -1, :])\n",
    "\n",
    "# FAN mejorado con más capas, mayor dimensionalidad y dropout\n",
    "class EnhancedFAN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, num_layers=4, output_size=1, dropout_rate=0.2):\n",
    "        super(EnhancedFAN, self).__init__()\n",
    "        \n",
    "        d_p = hidden_size // 3  # Mayor dimensión periódica\n",
    "        d_p_bar = hidden_size // 2  # Mantener dimensión no periódica\n",
    "        total_dim = 2 * d_p + d_p_bar\n",
    "        \n",
    "        # Primera capa\n",
    "        self.layer1 = FANLayer(input_size, d_p, d_p_bar)\n",
    "        \n",
    "        # Capas intermedias más profundas\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(FANLayer(total_dim, d_p, d_p_bar))\n",
    "        \n",
    "        # Capa final\n",
    "        self.output_layer = nn.Linear(total_dim, output_size)\n",
    "        \n",
    "        # Dropout para regularización\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = self.dropout(x)  # Aplicar dropout antes de cada capa\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Tomar el último paso temporal\n",
    "        last_step = x[:, -1, :]\n",
    "        return self.output_layer(last_step)\n",
    "\n",
    "# Modelo FAN con Atención\n",
    "class AttentionFAN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, num_layers=4, output_size=1, dropout_rate=0.2):\n",
    "        super(AttentionFAN, self).__init__()\n",
    "        \n",
    "        d_p = hidden_size // 3\n",
    "        d_p_bar = hidden_size // 2\n",
    "        total_dim = 2 * d_p + d_p_bar\n",
    "        \n",
    "        # Primera capa\n",
    "        self.layer1 = FANLayer(input_size, d_p, d_p_bar)\n",
    "        \n",
    "        # Capas intermedias\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.layers.append(FANLayer(total_dim, d_p, d_p_bar))\n",
    "        \n",
    "        # Mecanismo de atención\n",
    "        self.attention_query = nn.Linear(total_dim, total_dim)\n",
    "        self.attention_key = nn.Linear(total_dim, total_dim)\n",
    "        self.attention_value = nn.Linear(total_dim, total_dim)\n",
    "        \n",
    "        # Capa final\n",
    "        self.output_layer = nn.Linear(total_dim, output_size)\n",
    "        \n",
    "        # Dropout para regularización\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = self.dropout(x)\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Mecanismo de atención\n",
    "        query = self.attention_query(x[:, -1:, :])  # Usar último paso temporal como query\n",
    "        key = self.attention_key(x)\n",
    "        value = self.attention_value(x)\n",
    "        \n",
    "        # Calcular scores de atención\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (x.size(-1) ** 0.5)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Aplicar atención\n",
    "        context = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        # Usar el vector de contexto para la predicción\n",
    "        return self.output_layer(context.squeeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35760469-af4a-4bcc-b290-5d9e2431fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento mejorada SIN early stopping\n",
    "def train_model_improved(model, train_loader, val_loader, optimizer, criterion, num_epochs=100):\n",
    "\n",
    "    start_time = time.time()  # Iniciar contador de tiempo\n",
    "    #DEVICE = torch.device(\"mps\")  # Usar CPU para evitar problemas\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=5, factor=0.5, verbose=True\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradiente clipping para estabilidad\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validación\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)  # ← CORRECCIÓN AQUÍ\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Actualizar learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Guardar el mejor modelo, pero sin early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Restaurar el mejor modelo\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Al final, calcular tiempo transcurrido\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f} segundos ({training_time/60:.2f} minutos)\")\n",
    "    \n",
    "    return train_losses, val_losses, training_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9856969-43be-4a36-8f51-2060d3ecf747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear secuencias para entrenamiento\n",
    "def create_sequences(df, target_col, seq_length=24, horizon=1):\n",
    "    features = df.values\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(features) - seq_length - horizon + 1):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        y.append(features[i+seq_length:i+seq_length+horizon, df.columns.get_loc(target_col)])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "563ce9a4-cc21-4422-80d1-0b27af5da9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento del dataset\n",
    "def process_power_consumption_data(X, y):\n",
    "    # Combinar los datos de características y target\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "    \n",
    "    # Convertir la fecha y la hora a un índice de datetime\n",
    "    df['datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'], format='%d/%m/%Y %H:%M:%S')\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df.drop(['Date', 'Time'], axis=1, inplace=True)\n",
    "    \n",
    "    # Manejar valores perdidos\n",
    "    df.replace('?', np.nan, inplace=True)\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # Imputar valores faltantes\n",
    "    df = df.interpolate(method='time')\n",
    "    \n",
    "    # Resamplear a intervalo horario para reducir el tamaño del dataset\n",
    "    df_hourly = df.resample('H').mean()\n",
    "    \n",
    "    # Añadir características temporales\n",
    "    df_hourly['hour'] = df_hourly.index.hour\n",
    "    df_hourly['day'] = df_hourly.index.day\n",
    "    df_hourly['month'] = df_hourly.index.month\n",
    "    df_hourly['day_of_week'] = df_hourly.index.dayofweek\n",
    "    df_hourly['is_weekend'] = df_hourly['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Codificación cíclica para variables temporales\n",
    "    for col in ['hour', 'day', 'month', 'day_of_week']:\n",
    "        max_val = df_hourly[col].max() + 1\n",
    "        df_hourly[f'{col}_sin'] = np.sin(2 * np.pi * df_hourly[col] / max_val)\n",
    "        df_hourly[f'{col}_cos'] = np.cos(2 * np.pi * df_hourly[col] / max_val)\n",
    "        df_hourly = df_hourly.drop(col, axis=1)\n",
    "    \n",
    "    return df_hourly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3be32a45-7bb1-4f75-8fc4-522e441f3215",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3545194198.py, line 14)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdf = process_power_consumption_data(X, y)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Función principal\n",
    "def main():\n",
    "    print(\"Procesando los datos...\")\n",
    "    \n",
    "# Descargar el dataset\n",
    "    print(\"Descargando el dataset de consumo eléctrico...\")\n",
    "    individual_household_electric_power_consumption = fetch_ucirepo(id=235)\n",
    "    \n",
    "    # Extraer los datos\n",
    "    X = individual_household_electric_power_consumption.data.features\n",
    "    y = individual_household_electric_power_consumption.data.targets\n",
    "    \n",
    "    # Procesar el dataset\n",
    "    df = process_power_consumption_data(X, y)\n",
    "    \n",
    "    print(f\"\\nDatos procesados. Forma del dataframe: {df.shape}\")\n",
    "    \n",
    "    # Target y características\n",
    "    target_col = 'Global_active_power'\n",
    "    feature_cols = df.columns.tolist()\n",
    "    \n",
    "    # Normalización\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    \n",
    "    # Crear secuencias\n",
    "    print(\"\\nCreando secuencias para entrenamiento...\")\n",
    "    seq_length = 24  # 24 horas (1 día)\n",
    "    X_seq, y_seq = create_sequences(df_scaled, target_col, seq_length=seq_length, horizon=1)\n",
    "    print(f\"Forma de las secuencias - X: {X_seq.shape}, y: {y_seq.shape}\")\n",
    "    \n",
    "    # Dividir en train, val, test\n",
    "    test_size = 0.2\n",
    "    val_size = 0.2\n",
    "    \n",
    "    # Primero separamos el test\n",
    "    test_start_idx = int(len(X_seq) * (1 - test_size))\n",
    "    X_train_val, X_test = X_seq[:test_start_idx], X_seq[test_start_idx:]\n",
    "    y_train_val, y_test = y_seq[:test_start_idx], y_seq[test_start_idx:]\n",
    "    \n",
    "    # Luego separamos train y validación\n",
    "    val_start_idx = int(len(X_train_val) * (1 - val_size))\n",
    "    X_train, X_val = X_train_val[:val_start_idx], X_train_val[val_start_idx:]\n",
    "    y_train, y_val = y_train_val[:val_start_idx], y_train_val[val_start_idx:]\n",
    "    \n",
    "    print(f\"\\nDivisión del dataset:\")\n",
    "    print(f\"Train: {X_train.shape[0]} muestras\")\n",
    "    print(f\"Validación: {X_val.shape[0]} muestras\")\n",
    "    print(f\"Test: {X_test.shape[0]} muestras\")\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "    y_val_tensor = torch.FloatTensor(y_val)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.FloatTensor(y_test)\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    batch_size = 32 # CAMBIADO 64\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Parámetros de los modelos\n",
    "    input_size = X_seq.shape[2]  # Número de características\n",
    "    hidden_size = 64  # Incrementado de 64 a 256 CAMBIADO 128ok\n",
    "    num_layers = 4    # Incrementado de 2 a 4\n",
    "    dropout_rate = 0.2 # CAMBIADO, 0.2\n",
    "    \n",
    "    # Inicializar modelos\n",
    "    models = {\n",
    "        'LSTM': LSTMModel(input_size, hidden_size, num_layers),\n",
    "        'FAN': FAN(input_size, hidden_size, num_layers),\n",
    "        'GatedFAN': GatedFAN(input_size, hidden_size, num_layers),\n",
    "        'EnhancedFAN': EnhancedFAN(input_size, hidden_size, num_layers, dropout_rate=dropout_rate),\n",
    "        'AttentionFAN': AttentionFAN(input_size, hidden_size, num_layers, dropout_rate=dropout_rate)\n",
    "    }\n",
    "    \n",
    "    # Aplicar inicialización de pesos mejorada a todos los modelos\n",
    "    for name, model in models.items():\n",
    "        model.apply(init_weights)\n",
    "        print(f\"Inicialización de pesos mejorada aplicada al modelo {name}\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    model_history = {}\n",
    "    \n",
    "    # Entrenar modelos con el entrenador mejorado\n",
    "    num_epochs = 100  # Ajustado a 100 épocas\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEntrenando modelo {name}...\")\n",
    "        \n",
    "        # Ajustar hiperparámetros (learning rate más bajo y weight decay)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5) #CAMBIADO lr 0.0005\n",
    "        \n",
    "        # Usar función de entrenamiento mejorada sin early stopping\n",
    "        train_losses, val_losses, training_time = train_model_improved(\n",
    "            model, train_loader, val_loader, optimizer, criterion, \n",
    "            num_epochs=num_epochs\n",
    "        )\n",
    "        \n",
    "        model_history[name] = (train_losses, val_losses)\n",
    "    \n",
    "    # Visualizar curvas de pérdida con límite en el eje Y\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    for name, (train_losses, _) in model_history.items():\n",
    "        plt.plot(train_losses, label=f'{name} Train')\n",
    "    plt.title('Pérdida de Entrenamiento', fontsize=14)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Pérdida', fontsize=12)\n",
    "    #plt.ylim(0, 5)  # Limitar eje Y a máximo 10\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    for name, (_, val_losses) in model_history.items():\n",
    "        plt.plot(val_losses, label=f'{name} Val')\n",
    "    plt.title('Pérdida de Validación', fontsize=14)\n",
    "    plt.xlabel('Época', fontsize=12)\n",
    "    plt.ylabel('Pérdida', fontsize=12)\n",
    "    #plt.ylim(0, 5)  # Limitar eje Y a máximo 10\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('curvas_perdida.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluar en conjunto de prueba\n",
    "    test_predictions = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(DEVICE)  # Usar DEVICE global\n",
    "                outputs = model(X_batch).cpu().numpy()  # Asegurarse de que .cpu() está presente\n",
    "                all_preds.append(outputs)\n",
    "                all_actuals.append(y_batch.numpy())\n",
    "        \n",
    "        # Concatenar todas las predicciones y valores reales\n",
    "        predictions = np.vstack(all_preds).reshape(-1)\n",
    "        actuals = np.vstack(all_actuals).reshape(-1)\n",
    "        \n",
    "        # Invertir la normalización\n",
    "        col_idx = df.columns.get_loc(target_col)\n",
    "        pred_array = np.zeros((len(predictions), len(feature_cols)))\n",
    "        actual_array = np.zeros((len(actuals), len(feature_cols)))\n",
    "        \n",
    "        pred_array[:, col_idx] = predictions\n",
    "        actual_array[:, col_idx] = actuals\n",
    "        \n",
    "        pred_denorm = scaler.inverse_transform(pred_array)[:, col_idx]\n",
    "        actual_denorm = scaler.inverse_transform(actual_array)[:, col_idx]\n",
    "        \n",
    "        test_predictions[name] = (pred_denorm, actual_denorm)\n",
    "    \n",
    "    # Preparar datos para SARIMA mejorado\n",
    "    print(\"\\nPreparando datos para modelo SARIMA mejorado...\")\n",
    "\n",
    "    # Series temporales\n",
    "    train_target = df[target_col].iloc[:-len(y_test)]\n",
    "    test_target = df[target_col].iloc[-len(y_test):]\n",
    "\n",
    "    # Seleccionar solo las variables exógenas más importantes (hora del día y día de la semana)\n",
    "    # que tienen mayor influencia en el consumo eléctrico\n",
    "    important_exog_cols = ['hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', 'is_weekend']\n",
    "    filtered_exog_cols = [col for col in important_exog_cols if col in df.columns]\n",
    "\n",
    "    if filtered_exog_cols:\n",
    "        exog_train = df[filtered_exog_cols].iloc[:-len(y_test)]\n",
    "        exog_test = df[filtered_exog_cols].iloc[-len(y_test):]\n",
    "        print(f\"Usando {len(filtered_exog_cols)} variables exógenas clave para SARIMA: {filtered_exog_cols}\")\n",
    "    else:\n",
    "        exog_train = None\n",
    "        exog_test = None\n",
    "        print(\"No se encontraron las variables exógenas clave. Usando SARIMA sin exógenas.\")\n",
    "\n",
    "    # Entrenar modelo SARIMA mejorado y hacer predicciones\n",
    "    print(\"\\nEntrenando modelo SARIMA mejorado...\")\n",
    "    sarima_forecast, sarima_fit, sarima_time = train_and_predict_sarima_improved(train_target, test_target, exog_train, exog_test)\n",
    "\n",
    "    # Reemplazar las predicciones\n",
    "    arima_forecast = sarima_forecast\n",
    "    \n",
    "    # Calcular métricas para SARIMA\n",
    "    sarima_rmse = np.sqrt(mean_squared_error(test_target, sarima_forecast))\n",
    "    sarima_r2 = r2_score(test_target, sarima_forecast)\n",
    "\n",
    "    print(f\"\\nSARIMA - RMSE: {sarima_rmse:.4f}, R²: {sarima_r2:.4f}\")\n",
    "    \n",
    "    # Calcular el error del SARIMA en el conjunto de validación\n",
    "    val_target = df[target_col].iloc[-len(y_test)-len(y_val):-len(y_test)]\n",
    "    if filtered_exog_cols:\n",
    "        val_exog = df[filtered_exog_cols].iloc[-len(y_test)-len(y_val):-len(y_test)]\n",
    "        sarima_val_pred = sarima_fit.predict(start=len(train_target), end=len(train_target)+len(val_target)-1, exog=val_exog)\n",
    "    else:\n",
    "        sarima_val_pred = sarima_fit.predict(start=len(train_target), end=len(train_target)+len(val_target)-1)\n",
    "\n",
    "    sarima_val_loss = mean_squared_error(sarima_val_pred, val_target)\n",
    "    print(f\"SARIMA - Pérdida de validación: {sarima_val_loss:.4f}\")\n",
    "\n",
    "    # Recopilar pérdidas finales de entrenamiento y validación para todos los modelos\n",
    "    final_losses = {\n",
    "        'Modelo': [],\n",
    "        'Pérdida de Entrenamiento': [],\n",
    "        'Pérdida de Validación': []\n",
    "    }\n",
    "\n",
    "    # Añadir pérdidas de modelos de redes neuronales\n",
    "    for name, (train_losses, val_losses) in model_history.items():\n",
    "        final_losses['Modelo'].append(name)\n",
    "        final_losses['Pérdida de Entrenamiento'].append(train_losses[-1])\n",
    "        final_losses['Pérdida de Validación'].append(val_losses[-1])\n",
    "\n",
    "    # Añadir pérdidas de SARIMA\n",
    "    final_losses['Modelo'].append('SARIMA')\n",
    "    final_losses['Pérdida de Entrenamiento'].append(sarima_rmse**2)  # Usando el error de test como aproximación\n",
    "    final_losses['Pérdida de Validación'].append(sarima_val_loss)\n",
    "\n",
    "    # Convertir a DataFrame y visualizar\n",
    "    final_losses_df = pd.DataFrame(final_losses)\n",
    "    print(\"\\nPérdidas finales de todos los modelos:\")\n",
    "    print(final_losses_df)\n",
    "\n",
    "    # Visualizar comparación de pérdidas finales\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x='Modelo', y='Pérdida de Entrenamiento', data=final_losses_df)\n",
    "    plt.title('Pérdida de Entrenamiento Final', fontsize=14)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x='Modelo', y='Pérdida de Validación', data=final_losses_df)\n",
    "    plt.title('Pérdida de Validación Final', fontsize=14)\n",
    "    plt.ylabel('MSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparacion_perdidas_finales.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Fechas para el conjunto de test\n",
    "    test_dates = df.index[-len(y_test):]\n",
    "    \n",
    "    # Crear gráficos interactivos para predicciones de 1 semana\n",
    "    print(\"\\nGráfico interactivo - Predicción de Consumo Eléctrico (1 semana):\")\n",
    "    plot_interactive_predictions(test_dates, test_target, test_predictions, sarima_forecast, period=\"week\", widget_id=\"week\")\n",
    "    \n",
    "    # Crear gráficos interactivos para predicciones detalladas de 3 días\n",
    "    print(\"\\nGráfico interactivo - Detalle de la Predicción (3 días):\")\n",
    "    plot_interactive_predictions(test_dates, test_target, test_predictions, sarima_forecast, period=\"detail\", widget_id=\"detail\")\n",
    "    \n",
    "    # Métricas de evaluación (solo RMSE y R²)\n",
    "    print(\"\\nMétricas de evaluación:\")\n",
    "    eval_metrics = []\n",
    "    for name, (preds, actuals) in test_predictions.items():\n",
    "        rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "        r2 = r2_score(actuals, preds)\n",
    "        eval_metrics.append({'Modelo': name, 'RMSE': rmse, 'R²': r2})\n",
    "        print(f\"{name} - RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
    "    \n",
    "    # Agregar SARIMA a las métricas\n",
    "    eval_metrics.append({'Modelo': 'SARIMA', 'RMSE': sarima_rmse, 'R²': sarima_r2})\n",
    "    \n",
    "    # Convertir a DataFrame para visualización\n",
    "    eval_df = pd.DataFrame(eval_metrics)\n",
    "    \n",
    "    # Visualizar RMSE de todos los modelos\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Modelo', y='RMSE', data=eval_df)\n",
    "    plt.title('RMSE por Modelo', fontsize=14)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rmse_comparativo.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparar rendimiento de los diferentes modelos FAN (solo RMSE)\n",
    "    fan_models = ['FAN', 'GatedFAN', 'EnhancedFAN', 'AttentionFAN']\n",
    "    fan_metrics = {\n",
    "        'Modelo': [],\n",
    "        'RMSE': [],\n",
    "        'R²': []\n",
    "    }\n",
    "\n",
    "    # Recopilar métricas solo para modelos FAN\n",
    "    for name in fan_models:\n",
    "        if name in test_predictions:\n",
    "            preds, actuals = test_predictions[name]\n",
    "            rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "            r2 = r2_score(actuals, preds)\n",
    "            \n",
    "            fan_metrics['Modelo'].append(name)\n",
    "            fan_metrics['RMSE'].append(rmse)\n",
    "            fan_metrics['R²'].append(r2)\n",
    "\n",
    "    # Crear DataFrame y visualizar\n",
    "    fan_metrics_df = pd.DataFrame(fan_metrics)\n",
    "    print(\"\\nComparación de modelos FAN:\")\n",
    "    print(fan_metrics_df)\n",
    "\n",
    "    # Visualizar comparación de modelos FAN\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # RMSE\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x='Modelo', y='RMSE', data=fan_metrics_df)\n",
    "    plt.title('RMSE por Modelo FAN', fontsize=14)\n",
    "    plt.ylabel('RMSE', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # R²\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x='Modelo', y='R²', data=fan_metrics_df)\n",
    "    plt.title('R² por Modelo FAN', fontsize=14)\n",
    "    plt.ylabel('R²', fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('comparacion_modelos_fan.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Realizar análisis detallado de resultados\n",
    "    print(\"\\nRealizando análisis detallado de resultados...\")\n",
    "    \n",
    "    # Obtener fechas del test\n",
    "    test_dates = df.index[-len(y_test):]\n",
    "    \n",
    "    # Llamar a la función de análisis\n",
    "    metrics_df, error_df, extremes_df = analyze_results(\n",
    "        models=models,\n",
    "        test_predictions=test_predictions,\n",
    "        test_dates=test_dates,\n",
    "        test_target=test_target,\n",
    "        arima_forecast=arima_forecast\n",
    "    )\n",
    "    \n",
    "    # Guardar resultados en CSV para análisis posterior\n",
    "    metrics_df.to_csv('metricas_modelos.csv')\n",
    "    error_df.to_csv('errores_por_hora.csv')\n",
    "    extremes_df.to_csv('analisis_valores_extremos.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Science)",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
